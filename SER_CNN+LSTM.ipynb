{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNZeuXcWwz6Xj3cfFUbowxD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Animeshp43/Speech_Emotion_Recognition/blob/main/SER_CNN%2BLSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ip9yTcnK7zV9",
        "outputId": "58c11867-c2de-428e-96fc-85bbb1d8fcff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls \"/content/drive/MyDrive/SER_Audio_Dataset\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K_nN1wIG8Mmq",
        "outputId": "a5751753-a6ba-4a12-c0cd-0a3197696415"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Crema\t     prepare_dataset  Savee  X_features.npy\n",
            "New_dataset  Ravdess\t      Tess   y_labels.npy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls \"/content/drive/MyDrive/SER_Audio_Dataset\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U620sTR88WDe",
        "outputId": "56b06681-5cfb-4b63-c9d0-b3db54437973"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Crema  Ravdess\tSavee  Tess\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!find \"/content/drive/MyDrive/SER_Audio_Dataset/Crema\" -type f | head\n",
        "!find \"/content/drive/MyDrive/SER_Audio_Dataset/Ravdess\" -type f | head\n",
        "!find \"/content/drive/MyDrive/SER_Audio_Dataset/Savee\" -type f | head\n",
        "!find \"/content/drive/MyDrive/SER_Audio_Dataset/Tess\" -type f | head\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P2bxxGO28Y52",
        "outputId": "ada81dbc-9b33-4005-cb99-5c8830515fad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/SER_Audio_Dataset/Crema/1081_DFA_DIS_XX.wav\n",
            "/content/drive/MyDrive/SER_Audio_Dataset/Crema/1080_WSI_ANG_XX.wav\n",
            "/content/drive/MyDrive/SER_Audio_Dataset/Crema/1079_TAI_DIS_XX.wav\n",
            "/content/drive/MyDrive/SER_Audio_Dataset/Crema/1079_DFA_NEU_XX.wav\n",
            "/content/drive/MyDrive/SER_Audio_Dataset/Crema/1081_IEO_HAP_MD.wav\n",
            "/content/drive/MyDrive/SER_Audio_Dataset/Crema/1079_TSI_NEU_XX.wav\n",
            "/content/drive/MyDrive/SER_Audio_Dataset/Crema/1081_DFA_SAD_XX.wav\n",
            "/content/drive/MyDrive/SER_Audio_Dataset/Crema/1081_DFA_ANG_XX.wav\n",
            "/content/drive/MyDrive/SER_Audio_Dataset/Crema/1080_TIE_DIS_XX.wav\n",
            "/content/drive/MyDrive/SER_Audio_Dataset/Crema/1079_TAI_NEU_XX.wav\n",
            "/content/drive/MyDrive/SER_Audio_Dataset/Ravdess/audio_speech_actors_01-24/Actor_02/03-01-02-01-02-02-02.wav\n",
            "/content/drive/MyDrive/SER_Audio_Dataset/Ravdess/audio_speech_actors_01-24/Actor_02/03-01-01-01-01-02-02.wav\n",
            "/content/drive/MyDrive/SER_Audio_Dataset/Ravdess/audio_speech_actors_01-24/Actor_02/03-01-01-01-02-02-02.wav\n",
            "/content/drive/MyDrive/SER_Audio_Dataset/Ravdess/audio_speech_actors_01-24/Actor_02/03-01-03-02-02-01-02.wav\n",
            "/content/drive/MyDrive/SER_Audio_Dataset/Ravdess/audio_speech_actors_01-24/Actor_02/03-01-04-01-01-01-02.wav\n",
            "/content/drive/MyDrive/SER_Audio_Dataset/Ravdess/audio_speech_actors_01-24/Actor_02/03-01-04-01-02-01-02.wav\n",
            "/content/drive/MyDrive/SER_Audio_Dataset/Ravdess/audio_speech_actors_01-24/Actor_02/03-01-03-01-02-02-02.wav\n",
            "/content/drive/MyDrive/SER_Audio_Dataset/Ravdess/audio_speech_actors_01-24/Actor_02/03-01-03-01-02-01-02.wav\n",
            "/content/drive/MyDrive/SER_Audio_Dataset/Ravdess/audio_speech_actors_01-24/Actor_02/03-01-04-02-01-01-02.wav\n",
            "/content/drive/MyDrive/SER_Audio_Dataset/Ravdess/audio_speech_actors_01-24/Actor_02/03-01-02-01-02-01-02.wav\n",
            "/content/drive/MyDrive/SER_Audio_Dataset/Savee/DC_a03.wav\n",
            "/content/drive/MyDrive/SER_Audio_Dataset/Savee/DC_a07.wav\n",
            "/content/drive/MyDrive/SER_Audio_Dataset/Savee/DC_a04.wav\n",
            "/content/drive/MyDrive/SER_Audio_Dataset/Savee/DC_a05.wav\n",
            "/content/drive/MyDrive/SER_Audio_Dataset/Savee/DC_a02.wav\n",
            "/content/drive/MyDrive/SER_Audio_Dataset/Savee/DC_a11.wav\n",
            "/content/drive/MyDrive/SER_Audio_Dataset/Savee/DC_a10.wav\n",
            "/content/drive/MyDrive/SER_Audio_Dataset/Savee/DC_a08.wav\n",
            "/content/drive/MyDrive/SER_Audio_Dataset/Savee/DC_a01.wav\n",
            "/content/drive/MyDrive/SER_Audio_Dataset/Savee/DC_a09.wav\n",
            "/content/drive/MyDrive/SER_Audio_Dataset/Tess/OAF_Fear/OAF_mill_fear.wav\n",
            "/content/drive/MyDrive/SER_Audio_Dataset/Tess/OAF_Fear/OAF_bath_fear.wav\n",
            "/content/drive/MyDrive/SER_Audio_Dataset/Tess/OAF_Fear/OAF_death_fear.wav\n",
            "/content/drive/MyDrive/SER_Audio_Dataset/Tess/OAF_Fear/OAF_bite_fear.wav\n",
            "/content/drive/MyDrive/SER_Audio_Dataset/Tess/OAF_Fear/OAF_match_fear.wav\n",
            "/content/drive/MyDrive/SER_Audio_Dataset/Tess/OAF_Fear/OAF_five_fear.wav\n",
            "/content/drive/MyDrive/SER_Audio_Dataset/Tess/OAF_Fear/OAF_fit_fear.wav\n",
            "/content/drive/MyDrive/SER_Audio_Dataset/Tess/OAF_Fear/OAF_cool_fear.wav\n",
            "/content/drive/MyDrive/SER_Audio_Dataset/Tess/OAF_Fear/OAF_puff_fear.wav\n",
            "/content/drive/MyDrive/SER_Audio_Dataset/Tess/OAF_Fear/OAF_make_fear.wav\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install librosa soundfile numpy pandas scikit-learn matplotlib keras tensorflow\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7jRY50gq8dbh",
        "outputId": "f8d02b42-ae57-4439-b9d5-0f1984a239e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: librosa in /usr/local/lib/python3.11/dist-packages (0.11.0)\n",
            "Requirement already satisfied: soundfile in /usr/local/lib/python3.11/dist-packages (0.13.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.11/dist-packages (3.8.0)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.11/dist-packages (from librosa) (3.0.1)\n",
            "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.60.0)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.15.3)\n",
            "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.5.0)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (4.4.2)\n",
            "Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.8.2)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.5.0.post1)\n",
            "Requirement already satisfied: typing_extensions>=4.1.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (4.13.2)\n",
            "Requirement already satisfied: lazy_loader>=0.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.4)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.1.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.11/dist-packages (from soundfile) (1.17.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.58.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from keras) (1.4.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras) (0.0.9)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.11/dist-packages (from keras) (3.13.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras) (0.15.0)\n",
            "Requirement already satisfied: ml-dtypes in /usr/local/lib/python3.11/dist-packages (from keras) (0.4.1)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (5.29.4)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.71.0)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0->soundfile) (2.22)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>=0.51.0->librosa) (0.43.0)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from pooch>=1.1->librosa) (4.3.8)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.4.26)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.8)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras) (2.19.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras) (0.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Manually count all .wav files recursively\n",
        "def count_audio_files(folder):\n",
        "    count = 0\n",
        "    for root, _, files in os.walk(folder):\n",
        "        count += len([f for f in files if f.endswith(\".wav\")])\n",
        "    return count\n",
        "\n",
        "folders = {\n",
        "    \"Crema\": \"/content/drive/MyDrive/SER_Audio_Dataset/Crema\",\n",
        "    \"Ravdess\": \"/content/drive/MyDrive/SER_Audio_Dataset/Ravdess/audio_speech_actors_01-24\",\n",
        "    \"Savee\": \"/content/drive/MyDrive/SER_Audio_Dataset/Savee\",\n",
        "    \"Tess\": \"/content/drive/MyDrive/SER_Audio_Dataset/Tess\"\n",
        "}\n",
        "\n",
        "total = 0\n",
        "for name, path in folders.items():\n",
        "    count = count_audio_files(path)\n",
        "    print(f\"{name}: 🎧 {count} audio files found\")\n",
        "    total += count\n",
        "\n",
        "print(f\"\\n📦 Total audio files across all datasets: {total}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WyBeCIJo9JXE",
        "outputId": "14ff3c3a-84a1-4720-cb02-9947040f6e1c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Crema: 🎧 7442 audio files found\n",
            "Ravdess: 🎧 1440 audio files found\n",
            "Savee: 🎧 480 audio files found\n",
            "Tess: 🎧 2800 audio files found\n",
            "\n",
            "📦 Total audio files across all datasets: 12162\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import librosa\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from collections import defaultdict, Counter\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Paths\n",
        "crema_path = \"/content/drive/MyDrive/SER_Audio_Dataset/Crema\"\n",
        "ravdess_path = \"/content/drive/MyDrive/SER_Audio_Dataset/Ravdess/audio_speech_actors_01-24\"\n",
        "savee_path = \"/content/drive/MyDrive/SER_Audio_Dataset/Savee\"\n",
        "tess_path = \"/content/drive/MyDrive/SER_Audio_Dataset/Tess\"\n",
        "\n",
        "# Emotion Mappings\n",
        "ravdess_emotions = {\n",
        "    '01': 'neutral', '02': 'calm', '03': 'happy', '04': 'sad',\n",
        "    '05': 'angry', '06': 'fear', '07': 'disgust', '08': 'surprise'\n",
        "}\n",
        "\n",
        "crema_emotions = {\n",
        "    'NEU': 'neutral', 'HAP': 'happy', 'SAD': 'sad', 'ANG': 'angry',\n",
        "    'FEA': 'fear', 'DIS': 'disgust'\n",
        "}\n",
        "\n",
        "savee_emotions = {\n",
        "    'a': 'angry', 'd': 'disgust', 'f': 'fear',\n",
        "    'h': 'happy', 'n': 'neutral', 'sa': 'sad', 'su': 'surprise'\n",
        "}\n",
        "\n",
        "tess_emotions = {\n",
        "    'angry': 'angry', 'disgust': 'disgust', 'fear': 'fear',\n",
        "    'happy': 'happy', 'neutral': 'neutral',\n",
        "    'pleasant_surprise': 'surprise', 'sad': 'sad'\n",
        "}\n",
        "\n",
        "# Constants\n",
        "N_MFCC = 40\n",
        "MAX_LEN = 173  # Number of time steps\n",
        "\n",
        "# Store features and labels\n",
        "features = []\n",
        "labels = []\n",
        "dataset_counts = defaultdict(int)\n",
        "\n",
        "# Function to extract padded MFCC sequences\n",
        "def extract_mfcc_sequence(file_path):\n",
        "    audio, sr = librosa.load(file_path, sr=None)\n",
        "    mfcc = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=N_MFCC)\n",
        "    if mfcc.shape[1] > MAX_LEN:\n",
        "        mfcc = mfcc[:, :MAX_LEN]\n",
        "    mfcc_padded = pad_sequences([mfcc.T], maxlen=MAX_LEN, padding='post', dtype='float32')[0]\n",
        "    return mfcc_padded\n",
        "\n",
        "# --- RAVDESS ---\n",
        "print(\"🔍 Processing Ravdess...\")\n",
        "for actor_folder in tqdm(sorted(os.listdir(ravdess_path))):\n",
        "    actor_path = os.path.join(ravdess_path, actor_folder)\n",
        "    for file in os.listdir(actor_path):\n",
        "        if file.endswith(\".wav\"):\n",
        "            emotion_code = file.split(\"-\")[2]\n",
        "            emotion = ravdess_emotions.get(emotion_code)\n",
        "            if emotion:\n",
        "                path = os.path.join(actor_path, file)\n",
        "                features.append(extract_mfcc_sequence(path))\n",
        "                labels.append(emotion)\n",
        "                dataset_counts[\"Ravdess\"] += 1\n",
        "\n",
        "# --- CREMA-D ---\n",
        "print(\"🔍 Processing Crema...\")\n",
        "for file in tqdm(sorted(os.listdir(crema_path))):\n",
        "    if file.endswith(\".wav\"):\n",
        "        parts = file.split('_')\n",
        "        emotion_code = parts[2]\n",
        "        emotion = crema_emotions.get(emotion_code)\n",
        "        if emotion:\n",
        "            path = os.path.join(crema_path, file)\n",
        "            features.append(extract_mfcc_sequence(path))\n",
        "            labels.append(emotion)\n",
        "            dataset_counts[\"Crema\"] += 1\n",
        "\n",
        "# --- SAVEE ---\n",
        "print(\"🔍 Processing Savee...\")\n",
        "for file in tqdm(sorted(os.listdir(savee_path))):\n",
        "    if file.endswith(\".wav\"):\n",
        "        file_name = file.split('.')[0]\n",
        "        emo_code = file_name.split('_')[-1]\n",
        "        matched = False\n",
        "        for key in sorted(savee_emotions.keys(), key=lambda x: -len(x)):\n",
        "            if emo_code.startswith(key):\n",
        "                emotion = savee_emotions[key]\n",
        "                path = os.path.join(savee_path, file)\n",
        "                features.append(extract_mfcc_sequence(path))\n",
        "                labels.append(emotion)\n",
        "                dataset_counts[\"Savee\"] += 1\n",
        "                matched = True\n",
        "                break\n",
        "        if not matched:\n",
        "            print(f\"⚠️ Skipped SAVEe file: {file} (unknown emotion code)\")\n",
        "\n",
        "# --- TESS ---\n",
        "print(\"🔍 Processing Tess...\")\n",
        "for folder in sorted(os.listdir(tess_path)):\n",
        "    folder_path = os.path.join(tess_path, folder)\n",
        "    if not os.path.isdir(folder_path):\n",
        "        continue\n",
        "    folder_lower = folder.lower().replace(\"-\", \"_\")\n",
        "    matched = False\n",
        "    for key in tess_emotions:\n",
        "        if key in folder_lower:\n",
        "            emotion = tess_emotions[key]\n",
        "            matched = True\n",
        "            break\n",
        "    if not matched:\n",
        "        print(f\"⚠️ Skipped folder: {folder}\")\n",
        "        continue\n",
        "    for file in os.listdir(folder_path):\n",
        "        if file.endswith(\".wav\"):\n",
        "            path = os.path.join(folder_path, file)\n",
        "            features.append(extract_mfcc_sequence(path))\n",
        "            labels.append(emotion)\n",
        "            dataset_counts[\"Tess\"] += 1\n",
        "\n",
        "# Final arrays\n",
        "X = np.array(features)\n",
        "y = np.array(labels)\n",
        "\n",
        "# Summary\n",
        "print(\"\\n📊 Dataset-wise Summary:\")\n",
        "for name, count in dataset_counts.items():\n",
        "    print(f\"{name}: ✅ {count} files loaded\")\n",
        "print(f\"\\n✅ Total files loaded: {len(X)}\")\n",
        "print(f\"✅ Feature shape: {X.shape}\")  # should be (samples, 173, 40)\n",
        "print(f\"✅ Label shape: {y.shape}\")\n",
        "print(\"\\n🔎 Emotion label distribution:\")\n",
        "print(Counter(y))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WC_8fe1k88-_",
        "outputId": "5873f706-cde3-4f30-dffe-985a9d477972"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔍 Processing Ravdess...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 24/24 [00:58<00:00,  2.43s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔍 Processing Crema...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 7442/7442 [04:32<00:00, 27.27it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔍 Processing Savee...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 480/480 [00:19<00:00, 25.14it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔍 Processing Tess...\n",
            "\n",
            "📊 Dataset-wise Summary:\n",
            "Ravdess: ✅ 1440 files loaded\n",
            "Crema: ✅ 7442 files loaded\n",
            "Savee: ✅ 480 files loaded\n",
            "Tess: ✅ 2800 files loaded\n",
            "\n",
            "✅ Total files loaded: 12162\n",
            "✅ Feature shape: (12162, 173, 40)\n",
            "✅ Label shape: (12162,)\n",
            "\n",
            "🔎 Emotion label distribution:\n",
            "Counter({np.str_('happy'): 1923, np.str_('sad'): 1923, np.str_('angry'): 1923, np.str_('disgust'): 1923, np.str_('fear'): 1923, np.str_('neutral'): 1703, np.str_('surprise'): 652, np.str_('calm'): 192})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Save features and labels\n",
        "np.save(\"/content/drive/MyDrive/SER_Audio_Dataset/X_features.npy\", X)\n",
        "np.save(\"/content/drive/MyDrive/SER_Audio_Dataset/y_labels.npy\", y)\n",
        "\n",
        "print(\" Features and labels saved successfully!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ybrJxVW5OKIY",
        "outputId": "5a4f5216-d605-4995-9729-6a5699e1ee3d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Features and labels saved successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load features and labels\n",
        "X = np.load(\"/content/drive/MyDrive/SER_Audio_Dataset/X_features.npy\")\n",
        "y = np.load(\"/content/drive/MyDrive/SER_Audio_Dataset/y_labels.npy\")\n",
        "\n",
        "print(\" Loaded saved features and labels.\")\n",
        "print(\"X shape:\", X.shape)\n",
        "print(\"y shape:\", y.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w-IKyOcUObK1",
        "outputId": "0c5ae1db-83c8-492c-c58f-6545e405638d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Loaded saved features and labels.\n",
            "X shape: (12162, 173, 40)\n",
            "y shape: (12162,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Encode labels\n",
        "le = LabelEncoder()\n",
        "y_encoded = le.fit_transform(y)\n",
        "y_categorical = to_categorical(y_encoded)\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y_categorical,\n",
        "    test_size=0.2,\n",
        "    stratify=y_encoded,\n",
        "    random_state=42\n",
        ")\n"
      ],
      "metadata": {
        "id": "nRqaqus76aMN"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv1D, MaxPooling1D, LSTM, Dropout, Dense, BatchNormalization\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "# CNN layers\n",
        "model.add(Conv1D(128, kernel_size=5, activation='relu', input_shape=(173, 40)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling1D(pool_size=2))\n",
        "model.add(Dropout(0.3))\n",
        "\n",
        "model.add(Conv1D(64, kernel_size=3, activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling1D(pool_size=2))\n",
        "model.add(Dropout(0.3))\n",
        "\n",
        "# LSTM\n",
        "model.add(LSTM(64, return_sequences=False))\n",
        "model.add(Dropout(0.3))\n",
        "\n",
        "# Dense output\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(y_categorical.shape[1], activation='softmax'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "model.summary()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 633
        },
        "id": "nIkax2QP6jGj",
        "outputId": "735e0e5e-af56-4816-e7a8-0ef14ed634d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ conv1d (\u001b[38;5;33mConv1D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m169\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │        \u001b[38;5;34m25,728\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m169\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │           \u001b[38;5;34m512\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling1d (\u001b[38;5;33mMaxPooling1D\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m84\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m84\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv1d_1 (\u001b[38;5;33mConv1D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m82\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │        \u001b[38;5;34m24,640\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_1           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m82\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │           \u001b[38;5;34m256\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling1d_1 (\u001b[38;5;33mMaxPooling1D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m41\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m41\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │        \u001b[38;5;34m33,024\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m4,160\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m)              │           \u001b[38;5;34m520\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ conv1d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">169</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │        <span style=\"color: #00af00; text-decoration-color: #00af00\">25,728</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">169</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling1d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">84</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">84</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv1d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">82</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">24,640</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_1           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">82</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling1d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">41</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">41</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">33,024</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,160</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">520</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m88,840\u001b[0m (347.03 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">88,840</span> (347.03 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m88,456\u001b[0m (345.53 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">88,456</span> (345.53 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m384\u001b[0m (1.50 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">384</span> (1.50 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "early_stop = EarlyStopping(monitor='val_loss', patience=12, restore_best_weights=True)\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=6, verbose=1)\n",
        "\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    epochs=100,\n",
        "    batch_size=32,\n",
        "    validation_data=(X_test, y_test),\n",
        "    callbacks=[early_stop, reduce_lr]\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a7eJQqO36k7-",
        "outputId": "8282109d-86a8-46a6-f4ad-39d6453b66db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 81ms/step - accuracy: 0.1634 - loss: 1.9709 - val_accuracy: 0.2137 - val_loss: 1.8856 - learning_rate: 0.0010\n",
            "Epoch 2/100\n",
            "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 91ms/step - accuracy: 0.2218 - loss: 1.8407 - val_accuracy: 0.3465 - val_loss: 1.6003 - learning_rate: 0.0010\n",
            "Epoch 3/100\n",
            "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 79ms/step - accuracy: 0.3292 - loss: 1.6110 - val_accuracy: 0.4180 - val_loss: 1.4214 - learning_rate: 0.0010\n",
            "Epoch 4/100\n",
            "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 74ms/step - accuracy: 0.4143 - loss: 1.4465 - val_accuracy: 0.4875 - val_loss: 1.3197 - learning_rate: 0.0010\n",
            "Epoch 5/100\n",
            "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 77ms/step - accuracy: 0.4685 - loss: 1.3287 - val_accuracy: 0.5179 - val_loss: 1.2030 - learning_rate: 0.0010\n",
            "Epoch 6/100\n",
            "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 78ms/step - accuracy: 0.5269 - loss: 1.2062 - val_accuracy: 0.5573 - val_loss: 1.1412 - learning_rate: 0.0010\n",
            "Epoch 7/100\n",
            "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 73ms/step - accuracy: 0.5284 - loss: 1.2083 - val_accuracy: 0.5129 - val_loss: 1.1913 - learning_rate: 0.0010\n",
            "Epoch 8/100\n",
            "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 79ms/step - accuracy: 0.5650 - loss: 1.1336 - val_accuracy: 0.5569 - val_loss: 1.1032 - learning_rate: 0.0010\n",
            "Epoch 9/100\n",
            "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 77ms/step - accuracy: 0.5590 - loss: 1.1364 - val_accuracy: 0.5520 - val_loss: 1.0888 - learning_rate: 0.0010\n",
            "Epoch 10/100\n",
            "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 81ms/step - accuracy: 0.5717 - loss: 1.0864 - val_accuracy: 0.5758 - val_loss: 1.0745 - learning_rate: 0.0010\n",
            "Epoch 11/100\n",
            "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 74ms/step - accuracy: 0.5870 - loss: 1.0582 - val_accuracy: 0.5536 - val_loss: 1.1409 - learning_rate: 0.0010\n",
            "Epoch 12/100\n",
            "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 75ms/step - accuracy: 0.5901 - loss: 1.0540 - val_accuracy: 0.5853 - val_loss: 1.0413 - learning_rate: 0.0010\n",
            "Epoch 13/100\n",
            "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 81ms/step - accuracy: 0.5565 - loss: 1.1446 - val_accuracy: 0.6001 - val_loss: 1.0083 - learning_rate: 0.0010\n",
            "Epoch 14/100\n",
            "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 81ms/step - accuracy: 0.5961 - loss: 1.0205 - val_accuracy: 0.5898 - val_loss: 1.0233 - learning_rate: 0.0010\n",
            "Epoch 15/100\n",
            "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 78ms/step - accuracy: 0.6007 - loss: 1.0150 - val_accuracy: 0.6038 - val_loss: 1.0143 - learning_rate: 0.0010\n",
            "Epoch 16/100\n",
            "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 79ms/step - accuracy: 0.6201 - loss: 0.9767 - val_accuracy: 0.5923 - val_loss: 1.0870 - learning_rate: 0.0010\n",
            "Epoch 17/100\n",
            "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 79ms/step - accuracy: 0.5446 - loss: 1.1725 - val_accuracy: 0.6071 - val_loss: 1.0094 - learning_rate: 0.0010\n",
            "Epoch 18/100\n",
            "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 79ms/step - accuracy: 0.6129 - loss: 0.9911 - val_accuracy: 0.6108 - val_loss: 0.9902 - learning_rate: 0.0010\n",
            "Epoch 19/100\n",
            "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 85ms/step - accuracy: 0.6205 - loss: 0.9843 - val_accuracy: 0.6099 - val_loss: 1.0209 - learning_rate: 0.0010\n",
            "Epoch 20/100\n",
            "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 78ms/step - accuracy: 0.6181 - loss: 0.9875 - val_accuracy: 0.6141 - val_loss: 0.9731 - learning_rate: 0.0010\n",
            "Epoch 21/100\n",
            "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 77ms/step - accuracy: 0.6325 - loss: 0.9548 - val_accuracy: 0.6206 - val_loss: 0.9736 - learning_rate: 0.0010\n",
            "Epoch 22/100\n",
            "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 81ms/step - accuracy: 0.6424 - loss: 0.9199 - val_accuracy: 0.6276 - val_loss: 0.9477 - learning_rate: 0.0010\n",
            "Epoch 23/100\n",
            "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 81ms/step - accuracy: 0.6476 - loss: 0.9199 - val_accuracy: 0.6284 - val_loss: 0.9441 - learning_rate: 0.0010\n",
            "Epoch 24/100\n",
            "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 85ms/step - accuracy: 0.6527 - loss: 0.8973 - val_accuracy: 0.6445 - val_loss: 0.9244 - learning_rate: 0.0010\n",
            "Epoch 25/100\n",
            "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 87ms/step - accuracy: 0.6636 - loss: 0.8979 - val_accuracy: 0.6515 - val_loss: 0.9072 - learning_rate: 0.0010\n",
            "Epoch 26/100\n",
            "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 78ms/step - accuracy: 0.6502 - loss: 0.9216 - val_accuracy: 0.6227 - val_loss: 0.9797 - learning_rate: 0.0010\n",
            "Epoch 27/100\n",
            "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 79ms/step - accuracy: 0.6609 - loss: 0.8880 - val_accuracy: 0.6289 - val_loss: 0.9487 - learning_rate: 0.0010\n",
            "Epoch 28/100\n",
            "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 81ms/step - accuracy: 0.6591 - loss: 0.8928 - val_accuracy: 0.6605 - val_loss: 0.8922 - learning_rate: 0.0010\n",
            "Epoch 29/100\n",
            "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 79ms/step - accuracy: 0.6667 - loss: 0.8740 - val_accuracy: 0.6490 - val_loss: 0.9093 - learning_rate: 0.0010\n",
            "Epoch 30/100\n",
            "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 79ms/step - accuracy: 0.6699 - loss: 0.8595 - val_accuracy: 0.6289 - val_loss: 0.9421 - learning_rate: 0.0010\n",
            "Epoch 31/100\n",
            "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 81ms/step - accuracy: 0.6718 - loss: 0.8766 - val_accuracy: 0.6515 - val_loss: 0.9265 - learning_rate: 0.0010\n",
            "Epoch 32/100\n",
            "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 80ms/step - accuracy: 0.6855 - loss: 0.8386 - val_accuracy: 0.6630 - val_loss: 0.9143 - learning_rate: 0.0010\n",
            "Epoch 33/100\n",
            "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 79ms/step - accuracy: 0.6888 - loss: 0.8213 - val_accuracy: 0.6478 - val_loss: 0.9168 - learning_rate: 0.0010\n",
            "Epoch 34/100\n",
            "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 75ms/step - accuracy: 0.6804 - loss: 0.8403 - val_accuracy: 0.6708 - val_loss: 0.8907 - learning_rate: 0.0010\n",
            "Epoch 35/100\n",
            "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 80ms/step - accuracy: 0.7049 - loss: 0.7993 - val_accuracy: 0.6613 - val_loss: 0.8873 - learning_rate: 0.0010\n",
            "Epoch 36/100\n",
            "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 80ms/step - accuracy: 0.7046 - loss: 0.7956 - val_accuracy: 0.6519 - val_loss: 0.9036 - learning_rate: 0.0010\n",
            "Epoch 37/100\n",
            "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 81ms/step - accuracy: 0.6836 - loss: 0.8135 - val_accuracy: 0.6580 - val_loss: 0.9008 - learning_rate: 0.0010\n",
            "Epoch 38/100\n",
            "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 85ms/step - accuracy: 0.7001 - loss: 0.8062 - val_accuracy: 0.6646 - val_loss: 0.8986 - learning_rate: 0.0010\n",
            "Epoch 39/100\n",
            "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 79ms/step - accuracy: 0.7006 - loss: 0.7812 - val_accuracy: 0.6560 - val_loss: 0.9035 - learning_rate: 0.0010\n",
            "Epoch 40/100\n",
            "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 75ms/step - accuracy: 0.7037 - loss: 0.7964 - val_accuracy: 0.6695 - val_loss: 0.8943 - learning_rate: 0.0010\n",
            "Epoch 41/100\n",
            "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 80ms/step - accuracy: 0.7197 - loss: 0.7567 - val_accuracy: 0.6609 - val_loss: 0.8744 - learning_rate: 0.0010\n",
            "Epoch 42/100\n",
            "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 80ms/step - accuracy: 0.7169 - loss: 0.7514 - val_accuracy: 0.6605 - val_loss: 0.9199 - learning_rate: 0.0010\n",
            "Epoch 43/100\n",
            "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 74ms/step - accuracy: 0.7191 - loss: 0.7547 - val_accuracy: 0.6412 - val_loss: 0.9501 - learning_rate: 0.0010\n",
            "Epoch 44/100\n",
            "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 75ms/step - accuracy: 0.7166 - loss: 0.7487 - val_accuracy: 0.6420 - val_loss: 0.9510 - learning_rate: 0.0010\n",
            "Epoch 45/100\n",
            "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 80ms/step - accuracy: 0.7238 - loss: 0.7474 - val_accuracy: 0.6576 - val_loss: 0.8987 - learning_rate: 0.0010\n",
            "Epoch 46/100\n",
            "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 83ms/step - accuracy: 0.7299 - loss: 0.7270 - val_accuracy: 0.6716 - val_loss: 0.8784 - learning_rate: 0.0010\n",
            "Epoch 47/100\n",
            "\u001b[1m304/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - accuracy: 0.7239 - loss: 0.7343\n",
            "Epoch 47: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 82ms/step - accuracy: 0.7239 - loss: 0.7343 - val_accuracy: 0.6634 - val_loss: 0.9005 - learning_rate: 0.0010\n",
            "Epoch 48/100\n",
            "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 81ms/step - accuracy: 0.7499 - loss: 0.7040 - val_accuracy: 0.6704 - val_loss: 0.8708 - learning_rate: 5.0000e-04\n",
            "Epoch 49/100\n",
            "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 74ms/step - accuracy: 0.7529 - loss: 0.6667 - val_accuracy: 0.6728 - val_loss: 0.8635 - learning_rate: 5.0000e-04\n",
            "Epoch 50/100\n",
            "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 80ms/step - accuracy: 0.7513 - loss: 0.6701 - val_accuracy: 0.6765 - val_loss: 0.8858 - learning_rate: 5.0000e-04\n",
            "Epoch 51/100\n",
            "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 80ms/step - accuracy: 0.7560 - loss: 0.6566 - val_accuracy: 0.6704 - val_loss: 0.9012 - learning_rate: 5.0000e-04\n",
            "Epoch 52/100\n",
            "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 79ms/step - accuracy: 0.7587 - loss: 0.6435 - val_accuracy: 0.6856 - val_loss: 0.8776 - learning_rate: 5.0000e-04\n",
            "Epoch 53/100\n",
            "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 74ms/step - accuracy: 0.7603 - loss: 0.6448 - val_accuracy: 0.6823 - val_loss: 0.8966 - learning_rate: 5.0000e-04\n",
            "Epoch 54/100\n",
            "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 80ms/step - accuracy: 0.7577 - loss: 0.6237 - val_accuracy: 0.6815 - val_loss: 0.8736 - learning_rate: 5.0000e-04\n",
            "Epoch 55/100\n",
            "\u001b[1m304/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 76ms/step - accuracy: 0.7526 - loss: 0.6669\n",
            "Epoch 55: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 80ms/step - accuracy: 0.7527 - loss: 0.6668 - val_accuracy: 0.6823 - val_loss: 0.8780 - learning_rate: 5.0000e-04\n",
            "Epoch 56/100\n",
            "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 81ms/step - accuracy: 0.7820 - loss: 0.5887 - val_accuracy: 0.6827 - val_loss: 0.8650 - learning_rate: 2.5000e-04\n",
            "Epoch 57/100\n",
            "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 85ms/step - accuracy: 0.7817 - loss: 0.5925 - val_accuracy: 0.6798 - val_loss: 0.8766 - learning_rate: 2.5000e-04\n",
            "Epoch 58/100\n",
            "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 83ms/step - accuracy: 0.7791 - loss: 0.6055 - val_accuracy: 0.6880 - val_loss: 0.8739 - learning_rate: 2.5000e-04\n",
            "Epoch 59/100\n",
            "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 82ms/step - accuracy: 0.7833 - loss: 0.5882 - val_accuracy: 0.6876 - val_loss: 0.8675 - learning_rate: 2.5000e-04\n",
            "Epoch 60/100\n",
            "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 80ms/step - accuracy: 0.7923 - loss: 0.5638 - val_accuracy: 0.6864 - val_loss: 0.8718 - learning_rate: 2.5000e-04\n",
            "Epoch 61/100\n",
            "\u001b[1m304/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 0.7824 - loss: 0.6058\n",
            "Epoch 61: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
            "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 81ms/step - accuracy: 0.7824 - loss: 0.6056 - val_accuracy: 0.6819 - val_loss: 0.8774 - learning_rate: 2.5000e-04\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate Train Accuracy\n",
        "train_loss, train_acc = model.evaluate(X_train, y_train, verbose=0)\n",
        "print(f\"✅ Train Accuracy: {train_acc:.2%}\")\n",
        "\n",
        "# Evaluate Test Accuracy\n",
        "test_loss, test_acc = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(f\"✅ Test Accuracy: {test_acc:.2%}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7jSms6y9DTYn",
        "outputId": "e5001215-4fd5-4105-e200-5a5d8885c1f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Train Accuracy: 80.92%\n",
            "✅ Test Accuracy: 67.28%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Define the accuracies\n",
        "train_accuracy = 80.92\n",
        "test_accuracy = 67.28\n",
        "\n",
        "# Labels and values\n",
        "labels = ['Train Accuracy', 'Test Accuracy']\n",
        "accuracies = [train_accuracy, test_accuracy]\n",
        "\n",
        "# Plotting\n",
        "plt.figure(figsize=(6, 5))\n",
        "bars = plt.bar(labels, accuracies, color=['skyblue', 'salmon'])\n",
        "\n",
        "# Annotate values on top\n",
        "for bar in bars:\n",
        "    yval = bar.get_height()\n",
        "    plt.text(bar.get_x() + bar.get_width()/2.0, yval + 1, f'{yval:.2f}%', ha='center', va='bottom', fontsize=12)\n",
        "\n",
        "# Title and labels\n",
        "plt.title('CNN+LSTM Model')\n",
        "plt.ylim(0, 100)\n",
        "plt.ylabel('Accuracy (%)')\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "VfDlIBZJDH-n",
        "outputId": "1a651e23-702a-4c5d-e0c5-551887b51027",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 507
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 600x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save model in HDF5 format\n",
        "model.save(\"cnn_lstm_ser1_model.h5\")\n",
        "print(\"✅ Model saved successfully as cnn_lstm_ser_model.h5\")\n"
      ],
      "metadata": {
        "id": "VMKTiQIXE-OC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57581874-5fc8-47d0-f022-5fc0879b32d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Model saved successfully as cnn_lstm_ser_model.h5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "\n",
        "# Move to Google Drive folder\n",
        "shutil.move(\"cnn_lstm_ser1_model.h5\", \"/content/drive/MyDrive/cnn_lstm_ser_model.h5\")\n",
        "print(\"✅ Model uploaded to Google Drive.\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uaPZsOUjE1et",
        "outputId": "c38d2527-cc7f-4cca-be34-5ab704b461d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Model uploaded to Google Drive.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tensorflow.keras.models import load_model\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load model\n",
        "model = load_model(\"/content/drive/MyDrive/cnn_lstm_ser_model.h5\")\n",
        "\n",
        "# Load features, labels\n",
        "X = np.load(\"/content/drive/MyDrive/SER_Audio_Dataset/X_features.npy\")\n",
        "y = np.load(\"/content/drive/MyDrive/SER_Audio_Dataset/y_labels.npy\")\n",
        "\n",
        "# Encode labels\n",
        "le = LabelEncoder()\n",
        "y_encoded = le.fit_transform(y)\n",
        "label_map = dict(zip(le.classes_, range(len(le.classes_))))\n",
        "id2label = dict(zip(range(len(le.classes_)), le.classes_))\n"
      ],
      "metadata": {
        "id": "EhFFCnUOE-CB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "488aa2da-685c-41ba-daf0-2df762e460c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import librosa\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Initialize\n",
        "features = []\n",
        "labels = []\n",
        "meta = []\n",
        "dataset_counts = {\"Ravdess\": 0, \"Crema\": 0, \"Savee\": 0, \"Tess\": 0}\n",
        "\n",
        "# Emotion mapping\n",
        "emotion_map = {\n",
        "    '01': 'neutral', '02': 'calm', '03': 'happy', '04': 'sad',\n",
        "    '05': 'angry', '06': 'fearful', '07': 'disgust', '08': 'surprised'\n",
        "}\n",
        "\n",
        "def extract_mfcc_sequence(path, max_len=173):\n",
        "    y, sr = librosa.load(path, sr=16000)\n",
        "    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=40)\n",
        "    if mfcc.shape[1] < max_len:\n",
        "        pad_width = max_len - mfcc.shape[1]\n",
        "        mfcc = np.pad(mfcc, pad_width=((0, 0), (0, pad_width)), mode='constant')\n",
        "    else:\n",
        "        mfcc = mfcc[:, :max_len]\n",
        "    return mfcc.T\n",
        "\n",
        "# Dataset paths\n",
        "base_path = \"/content/drive/MyDrive/SER_Audio_Dataset\"\n",
        "ravdess_path = os.path.join(base_path, \"Ravdess\")\n",
        "crema_path = os.path.join(base_path, \"Crema\")\n",
        "savee_path = os.path.join(base_path, \"Savee\")\n",
        "tess_path = os.path.join(base_path, \"Tess\")\n",
        "\n",
        "# 🔁 RAVDESS (Only speech, not song)\n",
        "for root, _, files in os.walk(ravdess_path):\n",
        "    for file in files:\n",
        "        if file.endswith(\".wav\"):\n",
        "            path = os.path.join(root, file)\n",
        "            parts = file.split(\"-\")\n",
        "            if len(parts) == 7 and parts[1] == '01':  # '01' = speech, '02' = song\n",
        "                emotion_code = parts[2]\n",
        "                emotion = emotion_map.get(emotion_code)\n",
        "                if emotion:\n",
        "                    features.append(extract_mfcc_sequence(path))\n",
        "                    labels.append(emotion)\n",
        "                    meta.append({\"filename\": file, \"source\": \"ravdess\"})\n",
        "                    dataset_counts[\"Ravdess\"] += 1\n",
        "\n",
        "# 🔁 CREMA-D\n",
        "for file in os.listdir(crema_path):\n",
        "    if file.endswith(\".wav\"):\n",
        "        path = os.path.join(crema_path, file)\n",
        "        parts = file.split(\"_\")\n",
        "        if len(parts) >= 3:\n",
        "            emotion_code = parts[2]\n",
        "            emotion = {\n",
        "                'ANG': 'angry', 'DIS': 'disgust', 'FEA': 'fearful', 'HAP': 'happy',\n",
        "                'NEU': 'neutral', 'SAD': 'sad'\n",
        "            }.get(emotion_code)\n",
        "            if emotion:\n",
        "                features.append(extract_mfcc_sequence(path))\n",
        "                labels.append(emotion)\n",
        "                meta.append({\"filename\": file, \"source\": \"crema\"})\n",
        "                dataset_counts[\"Crema\"] += 1\n",
        "\n",
        "# 🔁 SAVEE (fixed for filename format like DC_a04.wav)\n",
        "for file in os.listdir(savee_path):\n",
        "    if file.endswith(\".wav\"):\n",
        "        path = os.path.join(savee_path, file)\n",
        "        parts = file.split('_')\n",
        "        if len(parts) >= 2:\n",
        "            code = parts[1][0].lower()  # get first character of the emotion code (e.g., 'a' in 'a04')\n",
        "            emotion = {\n",
        "                'a': 'angry', 'd': 'disgust', 'f': 'fearful',\n",
        "                'h': 'happy', 'n': 'neutral', 's': 'sad', 'u': 'surprised', 'z': 'surprised'\n",
        "            }.get(code)\n",
        "            if emotion:\n",
        "                features.append(extract_mfcc_sequence(path))\n",
        "                labels.append(emotion)\n",
        "                meta.append({\"filename\": file, \"source\": \"savee\"})\n",
        "                dataset_counts[\"Savee\"] += 1\n",
        "\n",
        "\n",
        "\n",
        "# 🔁 TESS\n",
        "for root, _, files in os.walk(tess_path):\n",
        "    for file in files:\n",
        "        if file.endswith(\".wav\"):\n",
        "            path = os.path.join(root, file)\n",
        "            parts = file.split(\"_\")\n",
        "            if len(parts) >= 3:\n",
        "                raw_emotion = parts[2].replace(\".wav\", \"\").lower()\n",
        "                mapped = {\n",
        "                    'angry': 'angry', 'disgust': 'disgust', 'fear': 'fearful',\n",
        "                    'happy': 'happy', 'neutral': 'neutral', 'ps': 'surprised', 'sad': 'sad'\n",
        "                }.get(raw_emotion)\n",
        "                if mapped:\n",
        "                    features.append(extract_mfcc_sequence(path))\n",
        "                    labels.append(mapped)\n",
        "                    meta.append({\"filename\": file, \"source\": \"tess\"})\n",
        "                    dataset_counts[\"Tess\"] += 1\n",
        "\n",
        "# ✅ Convert to arrays and save\n",
        "X = np.array(features)\n",
        "y = np.array(labels)\n",
        "meta_df = pd.DataFrame(meta)\n",
        "\n",
        "np.save(os.path.join(base_path, \"X_features.npy\"), X)\n",
        "np.save(os.path.join(base_path, \"y_labels.npy\"), y)\n",
        "meta_df.to_csv(os.path.join(base_path, \"ser_metadata.csv\"), index=False)\n",
        "\n",
        "print(\" Features and labels saved.\")\n",
        "print(\" Metadata saved as ser_metadata.csv.\")\n",
        "print(\" Dataset counts:\", dataset_counts)\n",
        "print(\" Total samples:\", len(y))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ds8ZkUcBdxCw",
        "outputId": "74a1a8cb-7cca-46e7-ab5d-7d875ff2a253"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Features and labels saved.\n",
            " Metadata saved as ser_metadata.csv.\n",
            " Dataset counts: {'Ravdess': 1440, 'Crema': 7442, 'Savee': 480, 'Tess': 2800}\n",
            " Total samples: 12162\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "# Assuming you used this during preprocessing\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# After label encoding\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(y)\n",
        "\n",
        "# Save label encoder\n",
        "with open(\"/content/drive/MyDrive/SER_Audio_Dataset/label_encoder.pkl\", \"wb\") as f:\n",
        "    pickle.dump(label_encoder, f)\n",
        "\n",
        "print(\"✅ Label encoder saved.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wog8QzkEy7fs",
        "outputId": "2c4df55b-1d7c-4643-c06e-0b394cdddada"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Label encoder saved.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import pickle\n",
        "\n",
        "# Load saved labels\n",
        "y = np.load(\"/content/drive/MyDrive/SER_Audio_Dataset/y_labels.npy\")\n",
        "\n",
        "# Recreate label encoder\n",
        "label_encoder = LabelEncoder()\n",
        "label_encoder.fit(y)\n",
        "\n",
        "# Save it\n",
        "with open(\"/content/drive/MyDrive/SER_Audio_Dataset/label_encoder.pkl\", \"wb\") as f:\n",
        "    pickle.dump(label_encoder, f)\n",
        "\n",
        "print(\"✅ Label encoder recreated and saved.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O49CHnTLy98-",
        "outputId": "73a0d8fc-f10e-4659-be37-d8f28ef26962"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Label encoder recreated and saved.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iP4YeMbpLLzI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import librosa\n",
        "import numpy as np\n",
        "import pickle\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "# Paths\n",
        "model_path = \"/content/drive/MyDrive/cnn_lstm_ser_model.h5\"\n",
        "encoder_path = \"/content/drive/MyDrive/SER_Audio_Dataset/label_encoder.pkl\"\n",
        "audio_file_path = \"/content/drive/MyDrive/SER_Audio_Dataset/Ravdess/audio_speech_actors_01-24/Actor_01/03-01-01-01-01-01-01.wav\"  # 🔁 Change this\n",
        "\n",
        "# Load model and encoder\n",
        "model = load_model(model_path)\n",
        "with open(encoder_path, \"rb\") as f:\n",
        "    label_encoder = pickle.load(f)\n",
        "\n",
        "# Extract MFCC\n",
        "def extract_mfcc_sequence(path, max_len=173):\n",
        "    y, sr = librosa.load(path, sr=16000)\n",
        "    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=40)\n",
        "    if mfcc.shape[1] < max_len:\n",
        "        pad_width = max_len - mfcc.shape[1]\n",
        "        mfcc = np.pad(mfcc, pad_width=((0, 0), (0, pad_width)), mode='constant')\n",
        "    else:\n",
        "        mfcc = mfcc[:, :max_len]\n",
        "    return mfcc.T\n",
        "\n",
        "# Load audio and preprocess\n",
        "mfcc = extract_mfcc_sequence(audio_file_path)\n",
        "mfcc = np.expand_dims(mfcc, axis=0)  # Add batch dimension\n",
        "\n",
        "# Predict\n",
        "y_pred_prob = model.predict(mfcc)\n",
        "predicted_index = np.argmax(y_pred_prob)\n",
        "predicted_label = label_encoder.inverse_transform([predicted_index])[0]\n",
        "\n",
        "print(\"🎙️ Audio File:\", os.path.basename(audio_file_path))\n",
        "print(\"🔮 Predicted Emotion:\", predicted_label)\n"
      ],
      "metadata": {
        "id": "1aeNObrYKund",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e207d18-539a-4cec-e336-2f7486915114"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 463ms/step\n",
            "🎙️ Audio File: 1001_IOM_HAP_XX.wav\n",
            "🔮 Predicted Emotion: happy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import librosa\n",
        "import pickle\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "# ✅ Define paths\n",
        "base_path = \"/content/drive/MyDrive/SER_Audio_Dataset\"  # Correct path for label_encoder.pkl and datasets\n",
        "model_path = \"/content/drive/MyDrive/cnn_lstm_ser_model.h5\"  # Path to the model\n",
        "\n",
        "# === Load model and encoder ===\n",
        "model = load_model(model_path)\n",
        "\n",
        "with open(os.path.join(base_path, \"label_encoder.pkl\"), \"rb\") as f:\n",
        "    label_encoder = pickle.load(f)\n",
        "\n",
        "\n",
        "# === Dataset paths and counts ===\n",
        "dataset_info = {\n",
        "    \"crema\": {\"path\": os.path.join(base_path, \"Crema\"), \"count\": 7442},\n",
        "    \"ravdess\": {\"path\": os.path.join(base_path, \"Ravdess\"), \"count\": 1440},\n",
        "    \"savee\": {\"path\": os.path.join(base_path, \"Savee\"), \"count\": 480},\n",
        "    \"tess\": {\"path\": os.path.join(base_path, \"Tess\"), \"count\": 2800}\n",
        "}\n",
        "\n",
        "def extract_mfcc_sequence(path, max_len=173):\n",
        "    y, sr = librosa.load(path, sr=16000)\n",
        "    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=40)\n",
        "    if mfcc.shape[1] < max_len:\n",
        "        pad_width = max_len - mfcc.shape[1]\n",
        "        mfcc = np.pad(mfcc, pad_width=((0, 0), (0, pad_width)), mode='constant')\n",
        "    else:\n",
        "        mfcc = mfcc[:, :max_len]\n",
        "    return mfcc.T\n",
        "\n",
        "# === User input ===\n",
        "dataset_name = input(\"Available datasets: ravdess, crema, savee, tess\\nEnter dataset name: \").strip().lower()\n",
        "\n",
        "if dataset_name not in dataset_info:\n",
        "    print(\"❌ Invalid dataset name.\")\n",
        "else:\n",
        "    dataset_path = dataset_info[dataset_name][\"path\"]\n",
        "    max_index = dataset_info[dataset_name][\"count\"] - 1\n",
        "\n",
        "    print(f\"✅ Found {max_index + 1} samples in {dataset_name}.\")\n",
        "    sample_index = int(input(f\"Enter sample number (0 to {max_index}): \").strip())\n",
        "\n",
        "    if not (0 <= sample_index <= max_index):\n",
        "        print(\"❌ Invalid sample number.\")\n",
        "    else:\n",
        "        # Get all .wav files in sorted order\n",
        "        audio_files = []\n",
        "        for root, _, files in os.walk(dataset_path):\n",
        "            wavs = sorted([os.path.join(root, f) for f in files if f.endswith(\".wav\")])\n",
        "            audio_files.extend(wavs)\n",
        "\n",
        "        audio_path = audio_files[sample_index]\n",
        "\n",
        "        # Predict\n",
        "        mfcc = extract_mfcc_sequence(audio_path)\n",
        "        mfcc = np.expand_dims(mfcc, axis=0)\n",
        "        prediction = model.predict(mfcc)\n",
        "        predicted_emotion = label_encoder.inverse_transform([np.argmax(prediction)])[0]\n",
        "\n",
        "        print(f\"🎧 File: {os.path.basename(audio_path)}\")\n",
        "        print(f\"🔮 Predicted Emotion: {predicted_emotion}\")\n"
      ],
      "metadata": {
        "id": "zoeqDmnnKzvD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "45b18030-75e0-4d77-db43-61546a48d82d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Available datasets: ravdess, crema, savee, tess\n",
            "Enter dataset name: tess\n",
            "✅ Found 2800 samples in tess.\n",
            "Enter sample number (0 to 2799): 3\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 354ms/step\n",
            "🎧 File: OAF_bath_fear.wav\n",
            "🔮 Predicted Emotion: fearful\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_mW8YvcNBClV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}