{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPx/q+StBpGEVAJ8eEFf0AX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Animeshp43/Speech_Emotion_Recognition/blob/main/SER_CNN%2BLSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ip9yTcnK7zV9",
        "outputId": "8d2c8bf0-51fe-460b-f645-7cefda934a2c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls \"/content/drive/MyDrive/SER_Audio_Dataset\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K_nN1wIG8Mmq",
        "outputId": "a5751753-a6ba-4a12-c0cd-0a3197696415"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Crema\t     prepare_dataset  Savee  X_features.npy\n",
            "New_dataset  Ravdess\t      Tess   y_labels.npy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls \"/content/drive/MyDrive/SER_Audio_Dataset\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U620sTR88WDe",
        "outputId": "56b06681-5cfb-4b63-c9d0-b3db54437973"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Crema  Ravdess\tSavee  Tess\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!find \"/content/drive/MyDrive/SER_Audio_Dataset/Crema\" -type f | head\n",
        "!find \"/content/drive/MyDrive/SER_Audio_Dataset/Ravdess\" -type f | head\n",
        "!find \"/content/drive/MyDrive/SER_Audio_Dataset/Savee\" -type f | head\n",
        "!find \"/content/drive/MyDrive/SER_Audio_Dataset/Tess\" -type f | head\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P2bxxGO28Y52",
        "outputId": "ada81dbc-9b33-4005-cb99-5c8830515fad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/SER_Audio_Dataset/Crema/1081_DFA_DIS_XX.wav\n",
            "/content/drive/MyDrive/SER_Audio_Dataset/Crema/1080_WSI_ANG_XX.wav\n",
            "/content/drive/MyDrive/SER_Audio_Dataset/Crema/1079_TAI_DIS_XX.wav\n",
            "/content/drive/MyDrive/SER_Audio_Dataset/Crema/1079_DFA_NEU_XX.wav\n",
            "/content/drive/MyDrive/SER_Audio_Dataset/Crema/1081_IEO_HAP_MD.wav\n",
            "/content/drive/MyDrive/SER_Audio_Dataset/Crema/1079_TSI_NEU_XX.wav\n",
            "/content/drive/MyDrive/SER_Audio_Dataset/Crema/1081_DFA_SAD_XX.wav\n",
            "/content/drive/MyDrive/SER_Audio_Dataset/Crema/1081_DFA_ANG_XX.wav\n",
            "/content/drive/MyDrive/SER_Audio_Dataset/Crema/1080_TIE_DIS_XX.wav\n",
            "/content/drive/MyDrive/SER_Audio_Dataset/Crema/1079_TAI_NEU_XX.wav\n",
            "/content/drive/MyDrive/SER_Audio_Dataset/Ravdess/audio_speech_actors_01-24/Actor_02/03-01-02-01-02-02-02.wav\n",
            "/content/drive/MyDrive/SER_Audio_Dataset/Ravdess/audio_speech_actors_01-24/Actor_02/03-01-01-01-01-02-02.wav\n",
            "/content/drive/MyDrive/SER_Audio_Dataset/Ravdess/audio_speech_actors_01-24/Actor_02/03-01-01-01-02-02-02.wav\n",
            "/content/drive/MyDrive/SER_Audio_Dataset/Ravdess/audio_speech_actors_01-24/Actor_02/03-01-03-02-02-01-02.wav\n",
            "/content/drive/MyDrive/SER_Audio_Dataset/Ravdess/audio_speech_actors_01-24/Actor_02/03-01-04-01-01-01-02.wav\n",
            "/content/drive/MyDrive/SER_Audio_Dataset/Ravdess/audio_speech_actors_01-24/Actor_02/03-01-04-01-02-01-02.wav\n",
            "/content/drive/MyDrive/SER_Audio_Dataset/Ravdess/audio_speech_actors_01-24/Actor_02/03-01-03-01-02-02-02.wav\n",
            "/content/drive/MyDrive/SER_Audio_Dataset/Ravdess/audio_speech_actors_01-24/Actor_02/03-01-03-01-02-01-02.wav\n",
            "/content/drive/MyDrive/SER_Audio_Dataset/Ravdess/audio_speech_actors_01-24/Actor_02/03-01-04-02-01-01-02.wav\n",
            "/content/drive/MyDrive/SER_Audio_Dataset/Ravdess/audio_speech_actors_01-24/Actor_02/03-01-02-01-02-01-02.wav\n",
            "/content/drive/MyDrive/SER_Audio_Dataset/Savee/DC_a03.wav\n",
            "/content/drive/MyDrive/SER_Audio_Dataset/Savee/DC_a07.wav\n",
            "/content/drive/MyDrive/SER_Audio_Dataset/Savee/DC_a04.wav\n",
            "/content/drive/MyDrive/SER_Audio_Dataset/Savee/DC_a05.wav\n",
            "/content/drive/MyDrive/SER_Audio_Dataset/Savee/DC_a02.wav\n",
            "/content/drive/MyDrive/SER_Audio_Dataset/Savee/DC_a11.wav\n",
            "/content/drive/MyDrive/SER_Audio_Dataset/Savee/DC_a10.wav\n",
            "/content/drive/MyDrive/SER_Audio_Dataset/Savee/DC_a08.wav\n",
            "/content/drive/MyDrive/SER_Audio_Dataset/Savee/DC_a01.wav\n",
            "/content/drive/MyDrive/SER_Audio_Dataset/Savee/DC_a09.wav\n",
            "/content/drive/MyDrive/SER_Audio_Dataset/Tess/OAF_Fear/OAF_mill_fear.wav\n",
            "/content/drive/MyDrive/SER_Audio_Dataset/Tess/OAF_Fear/OAF_bath_fear.wav\n",
            "/content/drive/MyDrive/SER_Audio_Dataset/Tess/OAF_Fear/OAF_death_fear.wav\n",
            "/content/drive/MyDrive/SER_Audio_Dataset/Tess/OAF_Fear/OAF_bite_fear.wav\n",
            "/content/drive/MyDrive/SER_Audio_Dataset/Tess/OAF_Fear/OAF_match_fear.wav\n",
            "/content/drive/MyDrive/SER_Audio_Dataset/Tess/OAF_Fear/OAF_five_fear.wav\n",
            "/content/drive/MyDrive/SER_Audio_Dataset/Tess/OAF_Fear/OAF_fit_fear.wav\n",
            "/content/drive/MyDrive/SER_Audio_Dataset/Tess/OAF_Fear/OAF_cool_fear.wav\n",
            "/content/drive/MyDrive/SER_Audio_Dataset/Tess/OAF_Fear/OAF_puff_fear.wav\n",
            "/content/drive/MyDrive/SER_Audio_Dataset/Tess/OAF_Fear/OAF_make_fear.wav\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install librosa soundfile numpy pandas scikit-learn matplotlib keras tensorflow\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7jRY50gq8dbh",
        "outputId": "f8d02b42-ae57-4439-b9d5-0f1984a239e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: librosa in /usr/local/lib/python3.11/dist-packages (0.11.0)\n",
            "Requirement already satisfied: soundfile in /usr/local/lib/python3.11/dist-packages (0.13.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.11/dist-packages (3.8.0)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.11/dist-packages (from librosa) (3.0.1)\n",
            "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.60.0)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.15.3)\n",
            "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.5.0)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (4.4.2)\n",
            "Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.8.2)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.5.0.post1)\n",
            "Requirement already satisfied: typing_extensions>=4.1.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (4.13.2)\n",
            "Requirement already satisfied: lazy_loader>=0.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.4)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.1.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.11/dist-packages (from soundfile) (1.17.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.58.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from keras) (1.4.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras) (0.0.9)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.11/dist-packages (from keras) (3.13.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras) (0.15.0)\n",
            "Requirement already satisfied: ml-dtypes in /usr/local/lib/python3.11/dist-packages (from keras) (0.4.1)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (5.29.4)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.71.0)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0->soundfile) (2.22)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>=0.51.0->librosa) (0.43.0)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from pooch>=1.1->librosa) (4.3.8)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.4.26)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.8)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras) (2.19.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras) (0.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Manually count all .wav files recursively\n",
        "def count_audio_files(folder):\n",
        "    count = 0\n",
        "    for root, _, files in os.walk(folder):\n",
        "        count += len([f for f in files if f.endswith(\".wav\")])\n",
        "    return count\n",
        "\n",
        "folders = {\n",
        "    \"Crema\": \"/content/drive/MyDrive/SER_Audio_Dataset/Crema\",\n",
        "    \"Ravdess\": \"/content/drive/MyDrive/SER_Audio_Dataset/Ravdess/audio_speech_actors_01-24\",\n",
        "    \"Savee\": \"/content/drive/MyDrive/SER_Audio_Dataset/Savee\",\n",
        "    \"Tess\": \"/content/drive/MyDrive/SER_Audio_Dataset/Tess\"\n",
        "}\n",
        "\n",
        "total = 0\n",
        "for name, path in folders.items():\n",
        "    count = count_audio_files(path)\n",
        "    print(f\"{name}: 🎧 {count} audio files found\")\n",
        "    total += count\n",
        "\n",
        "print(f\"\\n📦 Total audio files across all datasets: {total}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WyBeCIJo9JXE",
        "outputId": "14ff3c3a-84a1-4720-cb02-9947040f6e1c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Crema: 🎧 7442 audio files found\n",
            "Ravdess: 🎧 1440 audio files found\n",
            "Savee: 🎧 480 audio files found\n",
            "Tess: 🎧 2800 audio files found\n",
            "\n",
            "📦 Total audio files across all datasets: 12162\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import librosa\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from collections import defaultdict, Counter\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Paths\n",
        "crema_path = \"/content/drive/MyDrive/SER_Audio_Dataset/Crema\"\n",
        "ravdess_path = \"/content/drive/MyDrive/SER_Audio_Dataset/Ravdess/audio_speech_actors_01-24\"\n",
        "savee_path = \"/content/drive/MyDrive/SER_Audio_Dataset/Savee\"\n",
        "tess_path = \"/content/drive/MyDrive/SER_Audio_Dataset/Tess\"\n",
        "\n",
        "# Emotion Mappings\n",
        "ravdess_emotions = {\n",
        "    '01': 'neutral', '02': 'calm', '03': 'happy', '04': 'sad',\n",
        "    '05': 'angry', '06': 'fear', '07': 'disgust', '08': 'surprise'\n",
        "}\n",
        "\n",
        "crema_emotions = {\n",
        "    'NEU': 'neutral', 'HAP': 'happy', 'SAD': 'sad', 'ANG': 'angry',\n",
        "    'FEA': 'fear', 'DIS': 'disgust'\n",
        "}\n",
        "\n",
        "savee_emotions = {\n",
        "    'a': 'angry', 'd': 'disgust', 'f': 'fear',\n",
        "    'h': 'happy', 'n': 'neutral', 'sa': 'sad', 'su': 'surprise'\n",
        "}\n",
        "\n",
        "tess_emotions = {\n",
        "    'angry': 'angry', 'disgust': 'disgust', 'fear': 'fear',\n",
        "    'happy': 'happy', 'neutral': 'neutral',\n",
        "    'pleasant_surprise': 'surprise', 'sad': 'sad'\n",
        "}\n",
        "\n",
        "# Constants\n",
        "N_MFCC = 40\n",
        "MAX_LEN = 173  # Number of time steps\n",
        "\n",
        "# Store features and labels\n",
        "features = []\n",
        "labels = []\n",
        "dataset_counts = defaultdict(int)\n",
        "\n",
        "# Function to extract padded MFCC sequences\n",
        "def extract_mfcc_sequence(file_path):\n",
        "    audio, sr = librosa.load(file_path, sr=None)\n",
        "    mfcc = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=N_MFCC)\n",
        "    if mfcc.shape[1] > MAX_LEN:\n",
        "        mfcc = mfcc[:, :MAX_LEN]\n",
        "    mfcc_padded = pad_sequences([mfcc.T], maxlen=MAX_LEN, padding='post', dtype='float32')[0]\n",
        "    return mfcc_padded\n",
        "\n",
        "# --- RAVDESS ---\n",
        "print(\"🔍 Processing Ravdess...\")\n",
        "for actor_folder in tqdm(sorted(os.listdir(ravdess_path))):\n",
        "    actor_path = os.path.join(ravdess_path, actor_folder)\n",
        "    for file in os.listdir(actor_path):\n",
        "        if file.endswith(\".wav\"):\n",
        "            emotion_code = file.split(\"-\")[2]\n",
        "            emotion = ravdess_emotions.get(emotion_code)\n",
        "            if emotion:\n",
        "                path = os.path.join(actor_path, file)\n",
        "                features.append(extract_mfcc_sequence(path))\n",
        "                labels.append(emotion)\n",
        "                dataset_counts[\"Ravdess\"] += 1\n",
        "\n",
        "# --- CREMA-D ---\n",
        "print(\"🔍 Processing Crema...\")\n",
        "for file in tqdm(sorted(os.listdir(crema_path))):\n",
        "    if file.endswith(\".wav\"):\n",
        "        parts = file.split('_')\n",
        "        emotion_code = parts[2]\n",
        "        emotion = crema_emotions.get(emotion_code)\n",
        "        if emotion:\n",
        "            path = os.path.join(crema_path, file)\n",
        "            features.append(extract_mfcc_sequence(path))\n",
        "            labels.append(emotion)\n",
        "            dataset_counts[\"Crema\"] += 1\n",
        "\n",
        "# --- SAVEE ---\n",
        "print(\"🔍 Processing Savee...\")\n",
        "for file in tqdm(sorted(os.listdir(savee_path))):\n",
        "    if file.endswith(\".wav\"):\n",
        "        file_name = file.split('.')[0]\n",
        "        emo_code = file_name.split('_')[-1]\n",
        "        matched = False\n",
        "        for key in sorted(savee_emotions.keys(), key=lambda x: -len(x)):\n",
        "            if emo_code.startswith(key):\n",
        "                emotion = savee_emotions[key]\n",
        "                path = os.path.join(savee_path, file)\n",
        "                features.append(extract_mfcc_sequence(path))\n",
        "                labels.append(emotion)\n",
        "                dataset_counts[\"Savee\"] += 1\n",
        "                matched = True\n",
        "                break\n",
        "        if not matched:\n",
        "            print(f\"⚠️ Skipped SAVEe file: {file} (unknown emotion code)\")\n",
        "\n",
        "# --- TESS ---\n",
        "print(\"🔍 Processing Tess...\")\n",
        "for folder in sorted(os.listdir(tess_path)):\n",
        "    folder_path = os.path.join(tess_path, folder)\n",
        "    if not os.path.isdir(folder_path):\n",
        "        continue\n",
        "    folder_lower = folder.lower().replace(\"-\", \"_\")\n",
        "    matched = False\n",
        "    for key in tess_emotions:\n",
        "        if key in folder_lower:\n",
        "            emotion = tess_emotions[key]\n",
        "            matched = True\n",
        "            break\n",
        "    if not matched:\n",
        "        print(f\"⚠️ Skipped folder: {folder}\")\n",
        "        continue\n",
        "    for file in os.listdir(folder_path):\n",
        "        if file.endswith(\".wav\"):\n",
        "            path = os.path.join(folder_path, file)\n",
        "            features.append(extract_mfcc_sequence(path))\n",
        "            labels.append(emotion)\n",
        "            dataset_counts[\"Tess\"] += 1\n",
        "\n",
        "# Final arrays\n",
        "X = np.array(features)\n",
        "y = np.array(labels)\n",
        "\n",
        "# Summary\n",
        "print(\"\\n📊 Dataset-wise Summary:\")\n",
        "for name, count in dataset_counts.items():\n",
        "    print(f\"{name}: ✅ {count} files loaded\")\n",
        "print(f\"\\n✅ Total files loaded: {len(X)}\")\n",
        "print(f\"✅ Feature shape: {X.shape}\")  # should be (samples, 173, 40)\n",
        "print(f\"✅ Label shape: {y.shape}\")\n",
        "print(\"\\n🔎 Emotion label distribution:\")\n",
        "print(Counter(y))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WC_8fe1k88-_",
        "outputId": "5873f706-cde3-4f30-dffe-985a9d477972"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔍 Processing Ravdess...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 24/24 [00:58<00:00,  2.43s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔍 Processing Crema...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 7442/7442 [04:32<00:00, 27.27it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔍 Processing Savee...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 480/480 [00:19<00:00, 25.14it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔍 Processing Tess...\n",
            "\n",
            "📊 Dataset-wise Summary:\n",
            "Ravdess: ✅ 1440 files loaded\n",
            "Crema: ✅ 7442 files loaded\n",
            "Savee: ✅ 480 files loaded\n",
            "Tess: ✅ 2800 files loaded\n",
            "\n",
            "✅ Total files loaded: 12162\n",
            "✅ Feature shape: (12162, 173, 40)\n",
            "✅ Label shape: (12162,)\n",
            "\n",
            "🔎 Emotion label distribution:\n",
            "Counter({np.str_('happy'): 1923, np.str_('sad'): 1923, np.str_('angry'): 1923, np.str_('disgust'): 1923, np.str_('fear'): 1923, np.str_('neutral'): 1703, np.str_('surprise'): 652, np.str_('calm'): 192})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Save features and labels\n",
        "np.save(\"/content/drive/MyDrive/SER_Audio_Dataset/X_features.npy\", X)\n",
        "np.save(\"/content/drive/MyDrive/SER_Audio_Dataset/y_labels.npy\", y)\n",
        "\n",
        "print(\" Features and labels saved successfully!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ybrJxVW5OKIY",
        "outputId": "5845039b-fd50-4fbb-c271-133dbe0e8c3e"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Features and labels saved successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load features and labels\n",
        "X = np.load(\"/content/drive/MyDrive/SER_Audio_Dataset/X_features.npy\")\n",
        "y = np.load(\"/content/drive/MyDrive/SER_Audio_Dataset/y_labels.npy\")\n",
        "\n",
        "print(\" Loaded saved features and labels.\")\n",
        "print(\"X shape:\", X.shape)\n",
        "print(\"y shape:\", y.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w-IKyOcUObK1",
        "outputId": "e5784d17-9dec-47ee-e2fc-bf03993ad66b"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Loaded saved features and labels.\n",
            "X shape: (12162, 173, 40)\n",
            "y shape: (12162,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Encode labels\n",
        "le = LabelEncoder()\n",
        "y_encoded = le.fit_transform(y)\n",
        "y_categorical = to_categorical(y_encoded)\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y_categorical,\n",
        "    test_size=0.2,\n",
        "    stratify=y_encoded,\n",
        "    random_state=42\n",
        ")\n"
      ],
      "metadata": {
        "id": "nRqaqus76aMN"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv1D, MaxPooling1D, LSTM, Dropout, Dense, BatchNormalization\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "# CNN layers\n",
        "model.add(Conv1D(128, kernel_size=5, activation='relu', input_shape=(173, 40)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling1D(pool_size=2))\n",
        "model.add(Dropout(0.3))\n",
        "\n",
        "model.add(Conv1D(64, kernel_size=3, activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling1D(pool_size=2))\n",
        "model.add(Dropout(0.3))\n",
        "\n",
        "# LSTM\n",
        "model.add(LSTM(64, return_sequences=False))\n",
        "model.add(Dropout(0.3))\n",
        "\n",
        "# Dense output\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(y_categorical.shape[1], activation='softmax'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "model.summary()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 633
        },
        "id": "nIkax2QP6jGj",
        "outputId": "735e0e5e-af56-4816-e7a8-0ef14ed634d5"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ conv1d (\u001b[38;5;33mConv1D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m169\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │        \u001b[38;5;34m25,728\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m169\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │           \u001b[38;5;34m512\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling1d (\u001b[38;5;33mMaxPooling1D\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m84\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m84\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv1d_1 (\u001b[38;5;33mConv1D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m82\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │        \u001b[38;5;34m24,640\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_1           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m82\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │           \u001b[38;5;34m256\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling1d_1 (\u001b[38;5;33mMaxPooling1D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m41\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m41\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │        \u001b[38;5;34m33,024\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m4,160\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m)              │           \u001b[38;5;34m520\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ conv1d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">169</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │        <span style=\"color: #00af00; text-decoration-color: #00af00\">25,728</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">169</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling1d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">84</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">84</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv1d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">82</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">24,640</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_1           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">82</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling1d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">41</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">41</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">33,024</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,160</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">520</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m88,840\u001b[0m (347.03 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">88,840</span> (347.03 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m88,456\u001b[0m (345.53 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">88,456</span> (345.53 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m384\u001b[0m (1.50 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">384</span> (1.50 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "early_stop = EarlyStopping(monitor='val_loss', patience=12, restore_best_weights=True)\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=6, verbose=1)\n",
        "\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    epochs=100,\n",
        "    batch_size=32,\n",
        "    validation_data=(X_test, y_test),\n",
        "    callbacks=[early_stop, reduce_lr]\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a7eJQqO36k7-",
        "outputId": "8282109d-86a8-46a6-f4ad-39d6453b66db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 81ms/step - accuracy: 0.1634 - loss: 1.9709 - val_accuracy: 0.2137 - val_loss: 1.8856 - learning_rate: 0.0010\n",
            "Epoch 2/100\n",
            "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 91ms/step - accuracy: 0.2218 - loss: 1.8407 - val_accuracy: 0.3465 - val_loss: 1.6003 - learning_rate: 0.0010\n",
            "Epoch 3/100\n",
            "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 79ms/step - accuracy: 0.3292 - loss: 1.6110 - val_accuracy: 0.4180 - val_loss: 1.4214 - learning_rate: 0.0010\n",
            "Epoch 4/100\n",
            "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 74ms/step - accuracy: 0.4143 - loss: 1.4465 - val_accuracy: 0.4875 - val_loss: 1.3197 - learning_rate: 0.0010\n",
            "Epoch 5/100\n",
            "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 77ms/step - accuracy: 0.4685 - loss: 1.3287 - val_accuracy: 0.5179 - val_loss: 1.2030 - learning_rate: 0.0010\n",
            "Epoch 6/100\n",
            "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 78ms/step - accuracy: 0.5269 - loss: 1.2062 - val_accuracy: 0.5573 - val_loss: 1.1412 - learning_rate: 0.0010\n",
            "Epoch 7/100\n",
            "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 73ms/step - accuracy: 0.5284 - loss: 1.2083 - val_accuracy: 0.5129 - val_loss: 1.1913 - learning_rate: 0.0010\n",
            "Epoch 8/100\n",
            "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 79ms/step - accuracy: 0.5650 - loss: 1.1336 - val_accuracy: 0.5569 - val_loss: 1.1032 - learning_rate: 0.0010\n",
            "Epoch 9/100\n",
            "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 77ms/step - accuracy: 0.5590 - loss: 1.1364 - val_accuracy: 0.5520 - val_loss: 1.0888 - learning_rate: 0.0010\n",
            "Epoch 10/100\n",
            "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 81ms/step - accuracy: 0.5717 - loss: 1.0864 - val_accuracy: 0.5758 - val_loss: 1.0745 - learning_rate: 0.0010\n",
            "Epoch 11/100\n",
            "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 74ms/step - accuracy: 0.5870 - loss: 1.0582 - val_accuracy: 0.5536 - val_loss: 1.1409 - learning_rate: 0.0010\n",
            "Epoch 12/100\n",
            "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 75ms/step - accuracy: 0.5901 - loss: 1.0540 - val_accuracy: 0.5853 - val_loss: 1.0413 - learning_rate: 0.0010\n",
            "Epoch 13/100\n",
            "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 81ms/step - accuracy: 0.5565 - loss: 1.1446 - val_accuracy: 0.6001 - val_loss: 1.0083 - learning_rate: 0.0010\n",
            "Epoch 14/100\n",
            "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 81ms/step - accuracy: 0.5961 - loss: 1.0205 - val_accuracy: 0.5898 - val_loss: 1.0233 - learning_rate: 0.0010\n",
            "Epoch 15/100\n",
            "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 78ms/step - accuracy: 0.6007 - loss: 1.0150 - val_accuracy: 0.6038 - val_loss: 1.0143 - learning_rate: 0.0010\n",
            "Epoch 16/100\n",
            "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 79ms/step - accuracy: 0.6201 - loss: 0.9767 - val_accuracy: 0.5923 - val_loss: 1.0870 - learning_rate: 0.0010\n",
            "Epoch 17/100\n",
            "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 79ms/step - accuracy: 0.5446 - loss: 1.1725 - val_accuracy: 0.6071 - val_loss: 1.0094 - learning_rate: 0.0010\n",
            "Epoch 18/100\n",
            "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 79ms/step - accuracy: 0.6129 - loss: 0.9911 - val_accuracy: 0.6108 - val_loss: 0.9902 - learning_rate: 0.0010\n",
            "Epoch 19/100\n",
            "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 85ms/step - accuracy: 0.6205 - loss: 0.9843 - val_accuracy: 0.6099 - val_loss: 1.0209 - learning_rate: 0.0010\n",
            "Epoch 20/100\n",
            "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 78ms/step - accuracy: 0.6181 - loss: 0.9875 - val_accuracy: 0.6141 - val_loss: 0.9731 - learning_rate: 0.0010\n",
            "Epoch 21/100\n",
            "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 77ms/step - accuracy: 0.6325 - loss: 0.9548 - val_accuracy: 0.6206 - val_loss: 0.9736 - learning_rate: 0.0010\n",
            "Epoch 22/100\n",
            "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 81ms/step - accuracy: 0.6424 - loss: 0.9199 - val_accuracy: 0.6276 - val_loss: 0.9477 - learning_rate: 0.0010\n",
            "Epoch 23/100\n",
            "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 81ms/step - accuracy: 0.6476 - loss: 0.9199 - val_accuracy: 0.6284 - val_loss: 0.9441 - learning_rate: 0.0010\n",
            "Epoch 24/100\n",
            "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 85ms/step - accuracy: 0.6527 - loss: 0.8973 - val_accuracy: 0.6445 - val_loss: 0.9244 - learning_rate: 0.0010\n",
            "Epoch 25/100\n",
            "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 87ms/step - accuracy: 0.6636 - loss: 0.8979 - val_accuracy: 0.6515 - val_loss: 0.9072 - learning_rate: 0.0010\n",
            "Epoch 26/100\n",
            "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 78ms/step - accuracy: 0.6502 - loss: 0.9216 - val_accuracy: 0.6227 - val_loss: 0.9797 - learning_rate: 0.0010\n",
            "Epoch 27/100\n",
            "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 79ms/step - accuracy: 0.6609 - loss: 0.8880 - val_accuracy: 0.6289 - val_loss: 0.9487 - learning_rate: 0.0010\n",
            "Epoch 28/100\n",
            "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 81ms/step - accuracy: 0.6591 - loss: 0.8928 - val_accuracy: 0.6605 - val_loss: 0.8922 - learning_rate: 0.0010\n",
            "Epoch 29/100\n",
            "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 79ms/step - accuracy: 0.6667 - loss: 0.8740 - val_accuracy: 0.6490 - val_loss: 0.9093 - learning_rate: 0.0010\n",
            "Epoch 30/100\n",
            "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 79ms/step - accuracy: 0.6699 - loss: 0.8595 - val_accuracy: 0.6289 - val_loss: 0.9421 - learning_rate: 0.0010\n",
            "Epoch 31/100\n",
            "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 81ms/step - accuracy: 0.6718 - loss: 0.8766 - val_accuracy: 0.6515 - val_loss: 0.9265 - learning_rate: 0.0010\n",
            "Epoch 32/100\n",
            "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 80ms/step - accuracy: 0.6855 - loss: 0.8386 - val_accuracy: 0.6630 - val_loss: 0.9143 - learning_rate: 0.0010\n",
            "Epoch 33/100\n",
            "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 79ms/step - accuracy: 0.6888 - loss: 0.8213 - val_accuracy: 0.6478 - val_loss: 0.9168 - learning_rate: 0.0010\n",
            "Epoch 34/100\n",
            "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 75ms/step - accuracy: 0.6804 - loss: 0.8403 - val_accuracy: 0.6708 - val_loss: 0.8907 - learning_rate: 0.0010\n",
            "Epoch 35/100\n",
            "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 80ms/step - accuracy: 0.7049 - loss: 0.7993 - val_accuracy: 0.6613 - val_loss: 0.8873 - learning_rate: 0.0010\n",
            "Epoch 36/100\n",
            "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 80ms/step - accuracy: 0.7046 - loss: 0.7956 - val_accuracy: 0.6519 - val_loss: 0.9036 - learning_rate: 0.0010\n",
            "Epoch 37/100\n",
            "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 81ms/step - accuracy: 0.6836 - loss: 0.8135 - val_accuracy: 0.6580 - val_loss: 0.9008 - learning_rate: 0.0010\n",
            "Epoch 38/100\n",
            "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 85ms/step - accuracy: 0.7001 - loss: 0.8062 - val_accuracy: 0.6646 - val_loss: 0.8986 - learning_rate: 0.0010\n",
            "Epoch 39/100\n",
            "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 79ms/step - accuracy: 0.7006 - loss: 0.7812 - val_accuracy: 0.6560 - val_loss: 0.9035 - learning_rate: 0.0010\n",
            "Epoch 40/100\n",
            "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 75ms/step - accuracy: 0.7037 - loss: 0.7964 - val_accuracy: 0.6695 - val_loss: 0.8943 - learning_rate: 0.0010\n",
            "Epoch 41/100\n",
            "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 80ms/step - accuracy: 0.7197 - loss: 0.7567 - val_accuracy: 0.6609 - val_loss: 0.8744 - learning_rate: 0.0010\n",
            "Epoch 42/100\n",
            "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 80ms/step - accuracy: 0.7169 - loss: 0.7514 - val_accuracy: 0.6605 - val_loss: 0.9199 - learning_rate: 0.0010\n",
            "Epoch 43/100\n",
            "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 74ms/step - accuracy: 0.7191 - loss: 0.7547 - val_accuracy: 0.6412 - val_loss: 0.9501 - learning_rate: 0.0010\n",
            "Epoch 44/100\n",
            "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 75ms/step - accuracy: 0.7166 - loss: 0.7487 - val_accuracy: 0.6420 - val_loss: 0.9510 - learning_rate: 0.0010\n",
            "Epoch 45/100\n",
            "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 80ms/step - accuracy: 0.7238 - loss: 0.7474 - val_accuracy: 0.6576 - val_loss: 0.8987 - learning_rate: 0.0010\n",
            "Epoch 46/100\n",
            "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 83ms/step - accuracy: 0.7299 - loss: 0.7270 - val_accuracy: 0.6716 - val_loss: 0.8784 - learning_rate: 0.0010\n",
            "Epoch 47/100\n",
            "\u001b[1m304/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - accuracy: 0.7239 - loss: 0.7343\n",
            "Epoch 47: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 82ms/step - accuracy: 0.7239 - loss: 0.7343 - val_accuracy: 0.6634 - val_loss: 0.9005 - learning_rate: 0.0010\n",
            "Epoch 48/100\n",
            "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 81ms/step - accuracy: 0.7499 - loss: 0.7040 - val_accuracy: 0.6704 - val_loss: 0.8708 - learning_rate: 5.0000e-04\n",
            "Epoch 49/100\n",
            "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 74ms/step - accuracy: 0.7529 - loss: 0.6667 - val_accuracy: 0.6728 - val_loss: 0.8635 - learning_rate: 5.0000e-04\n",
            "Epoch 50/100\n",
            "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 80ms/step - accuracy: 0.7513 - loss: 0.6701 - val_accuracy: 0.6765 - val_loss: 0.8858 - learning_rate: 5.0000e-04\n",
            "Epoch 51/100\n",
            "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 80ms/step - accuracy: 0.7560 - loss: 0.6566 - val_accuracy: 0.6704 - val_loss: 0.9012 - learning_rate: 5.0000e-04\n",
            "Epoch 52/100\n",
            "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 79ms/step - accuracy: 0.7587 - loss: 0.6435 - val_accuracy: 0.6856 - val_loss: 0.8776 - learning_rate: 5.0000e-04\n",
            "Epoch 53/100\n",
            "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 74ms/step - accuracy: 0.7603 - loss: 0.6448 - val_accuracy: 0.6823 - val_loss: 0.8966 - learning_rate: 5.0000e-04\n",
            "Epoch 54/100\n",
            "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 80ms/step - accuracy: 0.7577 - loss: 0.6237 - val_accuracy: 0.6815 - val_loss: 0.8736 - learning_rate: 5.0000e-04\n",
            "Epoch 55/100\n",
            "\u001b[1m304/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 76ms/step - accuracy: 0.7526 - loss: 0.6669\n",
            "Epoch 55: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 80ms/step - accuracy: 0.7527 - loss: 0.6668 - val_accuracy: 0.6823 - val_loss: 0.8780 - learning_rate: 5.0000e-04\n",
            "Epoch 56/100\n",
            "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 81ms/step - accuracy: 0.7820 - loss: 0.5887 - val_accuracy: 0.6827 - val_loss: 0.8650 - learning_rate: 2.5000e-04\n",
            "Epoch 57/100\n",
            "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 85ms/step - accuracy: 0.7817 - loss: 0.5925 - val_accuracy: 0.6798 - val_loss: 0.8766 - learning_rate: 2.5000e-04\n",
            "Epoch 58/100\n",
            "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 83ms/step - accuracy: 0.7791 - loss: 0.6055 - val_accuracy: 0.6880 - val_loss: 0.8739 - learning_rate: 2.5000e-04\n",
            "Epoch 59/100\n",
            "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 82ms/step - accuracy: 0.7833 - loss: 0.5882 - val_accuracy: 0.6876 - val_loss: 0.8675 - learning_rate: 2.5000e-04\n",
            "Epoch 60/100\n",
            "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 80ms/step - accuracy: 0.7923 - loss: 0.5638 - val_accuracy: 0.6864 - val_loss: 0.8718 - learning_rate: 2.5000e-04\n",
            "Epoch 61/100\n",
            "\u001b[1m304/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 0.7824 - loss: 0.6058\n",
            "Epoch 61: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
            "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 81ms/step - accuracy: 0.7824 - loss: 0.6056 - val_accuracy: 0.6819 - val_loss: 0.8774 - learning_rate: 2.5000e-04\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate Train Accuracy\n",
        "train_loss, train_acc = model.evaluate(X_train, y_train, verbose=0)\n",
        "print(f\"✅ Train Accuracy: {train_acc:.2%}\")\n",
        "\n",
        "# Evaluate Test Accuracy\n",
        "test_loss, test_acc = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(f\"✅ Test Accuracy: {test_acc:.2%}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7jSms6y9DTYn",
        "outputId": "e5001215-4fd5-4105-e200-5a5d8885c1f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Train Accuracy: 80.92%\n",
            "✅ Test Accuracy: 67.28%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save model in HDF5 format\n",
        "model.save(\"cnn_lstm_ser1_model.h5\")\n",
        "print(\"✅ Model saved successfully as cnn_lstm_ser_model.h5\")\n"
      ],
      "metadata": {
        "id": "VMKTiQIXE-OC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57581874-5fc8-47d0-f022-5fc0879b32d3"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Model saved successfully as cnn_lstm_ser_model.h5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "\n",
        "# Move to Google Drive folder\n",
        "shutil.move(\"cnn_lstm_ser1_model.h5\", \"/content/drive/MyDrive/cnn_lstm_ser_model.h5\")\n",
        "print(\"✅ Model uploaded to Google Drive.\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uaPZsOUjE1et",
        "outputId": "c38d2527-cc7f-4cca-be34-5ab704b461d2"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Model uploaded to Google Drive.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tensorflow.keras.models import load_model\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load model\n",
        "model = load_model(\"/content/drive/MyDrive/cnn_lstm_ser_model.h5\")\n",
        "\n",
        "# Load features, labels\n",
        "X = np.load(\"/content/drive/MyDrive/SER_Audio_Dataset/X_features.npy\")\n",
        "y = np.load(\"/content/drive/MyDrive/SER_Audio_Dataset/y_labels.npy\")\n",
        "\n",
        "# Encode labels\n",
        "le = LabelEncoder()\n",
        "y_encoded = le.fit_transform(y)\n",
        "label_map = dict(zip(le.classes_, range(len(le.classes_))))\n",
        "id2label = dict(zip(range(len(le.classes_)), le.classes_))\n"
      ],
      "metadata": {
        "id": "EhFFCnUOE-CB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "488aa2da-685c-41ba-daf0-2df762e460c1"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import librosa\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Initialize\n",
        "features = []\n",
        "labels = []\n",
        "meta = []\n",
        "dataset_counts = {\"Ravdess\": 0, \"Crema\": 0, \"Savee\": 0, \"Tess\": 0}\n",
        "\n",
        "# Emotion mapping\n",
        "emotion_map = {\n",
        "    '01': 'neutral', '02': 'calm', '03': 'happy', '04': 'sad',\n",
        "    '05': 'angry', '06': 'fearful', '07': 'disgust', '08': 'surprised'\n",
        "}\n",
        "\n",
        "def extract_mfcc_sequence(path, max_len=173):\n",
        "    y, sr = librosa.load(path, sr=16000)\n",
        "    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=40)\n",
        "    if mfcc.shape[1] < max_len:\n",
        "        pad_width = max_len - mfcc.shape[1]\n",
        "        mfcc = np.pad(mfcc, pad_width=((0, 0), (0, pad_width)), mode='constant')\n",
        "    else:\n",
        "        mfcc = mfcc[:, :max_len]\n",
        "    return mfcc.T\n",
        "\n",
        "# Dataset paths\n",
        "base_path = \"/content/drive/MyDrive/SER_Audio_Dataset\"\n",
        "ravdess_path = os.path.join(base_path, \"Ravdess\")\n",
        "crema_path = os.path.join(base_path, \"Crema\")\n",
        "savee_path = os.path.join(base_path, \"Savee\")\n",
        "tess_path = os.path.join(base_path, \"Tess\")\n",
        "\n",
        "# 🔁 RAVDESS (Only speech, not song)\n",
        "for root, _, files in os.walk(ravdess_path):\n",
        "    for file in files:\n",
        "        if file.endswith(\".wav\"):\n",
        "            path = os.path.join(root, file)\n",
        "            parts = file.split(\"-\")\n",
        "            if len(parts) == 7 and parts[1] == '01':  # '01' = speech, '02' = song\n",
        "                emotion_code = parts[2]\n",
        "                emotion = emotion_map.get(emotion_code)\n",
        "                if emotion:\n",
        "                    features.append(extract_mfcc_sequence(path))\n",
        "                    labels.append(emotion)\n",
        "                    meta.append({\"filename\": file, \"source\": \"ravdess\"})\n",
        "                    dataset_counts[\"Ravdess\"] += 1\n",
        "\n",
        "# 🔁 CREMA-D\n",
        "for file in os.listdir(crema_path):\n",
        "    if file.endswith(\".wav\"):\n",
        "        path = os.path.join(crema_path, file)\n",
        "        parts = file.split(\"_\")\n",
        "        if len(parts) >= 3:\n",
        "            emotion_code = parts[2]\n",
        "            emotion = {\n",
        "                'ANG': 'angry', 'DIS': 'disgust', 'FEA': 'fearful', 'HAP': 'happy',\n",
        "                'NEU': 'neutral', 'SAD': 'sad'\n",
        "            }.get(emotion_code)\n",
        "            if emotion:\n",
        "                features.append(extract_mfcc_sequence(path))\n",
        "                labels.append(emotion)\n",
        "                meta.append({\"filename\": file, \"source\": \"crema\"})\n",
        "                dataset_counts[\"Crema\"] += 1\n",
        "\n",
        "# 🔁 SAVEE (fixed for filename format like DC_a04.wav)\n",
        "for file in os.listdir(savee_path):\n",
        "    if file.endswith(\".wav\"):\n",
        "        path = os.path.join(savee_path, file)\n",
        "        parts = file.split('_')\n",
        "        if len(parts) >= 2:\n",
        "            code = parts[1][0].lower()  # get first character of the emotion code (e.g., 'a' in 'a04')\n",
        "            emotion = {\n",
        "                'a': 'angry', 'd': 'disgust', 'f': 'fearful',\n",
        "                'h': 'happy', 'n': 'neutral', 's': 'sad', 'u': 'surprised', 'z': 'surprised'\n",
        "            }.get(code)\n",
        "            if emotion:\n",
        "                features.append(extract_mfcc_sequence(path))\n",
        "                labels.append(emotion)\n",
        "                meta.append({\"filename\": file, \"source\": \"savee\"})\n",
        "                dataset_counts[\"Savee\"] += 1\n",
        "\n",
        "\n",
        "\n",
        "# 🔁 TESS\n",
        "for root, _, files in os.walk(tess_path):\n",
        "    for file in files:\n",
        "        if file.endswith(\".wav\"):\n",
        "            path = os.path.join(root, file)\n",
        "            parts = file.split(\"_\")\n",
        "            if len(parts) >= 3:\n",
        "                raw_emotion = parts[2].replace(\".wav\", \"\").lower()\n",
        "                mapped = {\n",
        "                    'angry': 'angry', 'disgust': 'disgust', 'fear': 'fearful',\n",
        "                    'happy': 'happy', 'neutral': 'neutral', 'ps': 'surprised', 'sad': 'sad'\n",
        "                }.get(raw_emotion)\n",
        "                if mapped:\n",
        "                    features.append(extract_mfcc_sequence(path))\n",
        "                    labels.append(mapped)\n",
        "                    meta.append({\"filename\": file, \"source\": \"tess\"})\n",
        "                    dataset_counts[\"Tess\"] += 1\n",
        "\n",
        "# ✅ Convert to arrays and save\n",
        "X = np.array(features)\n",
        "y = np.array(labels)\n",
        "meta_df = pd.DataFrame(meta)\n",
        "\n",
        "np.save(os.path.join(base_path, \"X_features.npy\"), X)\n",
        "np.save(os.path.join(base_path, \"y_labels.npy\"), y)\n",
        "meta_df.to_csv(os.path.join(base_path, \"ser_metadata.csv\"), index=False)\n",
        "\n",
        "print(\" Features and labels saved.\")\n",
        "print(\" Metadata saved as ser_metadata.csv.\")\n",
        "print(\" Dataset counts:\", dataset_counts)\n",
        "print(\" Total samples:\", len(y))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ds8ZkUcBdxCw",
        "outputId": "74a1a8cb-7cca-46e7-ab5d-7d875ff2a253"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Features and labels saved.\n",
            " Metadata saved as ser_metadata.csv.\n",
            " Dataset counts: {'Ravdess': 1440, 'Crema': 7442, 'Savee': 480, 'Tess': 2800}\n",
            " Total samples: 12162\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "# Assuming you used this during preprocessing\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# After label encoding\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(y)\n",
        "\n",
        "# Save label encoder\n",
        "with open(\"/content/drive/MyDrive/SER_Audio_Dataset/label_encoder.pkl\", \"wb\") as f:\n",
        "    pickle.dump(label_encoder, f)\n",
        "\n",
        "print(\"✅ Label encoder saved.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wog8QzkEy7fs",
        "outputId": "2c4df55b-1d7c-4643-c06e-0b394cdddada"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Label encoder saved.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import pickle\n",
        "\n",
        "# Load saved labels\n",
        "y = np.load(\"/content/drive/MyDrive/SER_Audio_Dataset/y_labels.npy\")\n",
        "\n",
        "# Recreate label encoder\n",
        "label_encoder = LabelEncoder()\n",
        "label_encoder.fit(y)\n",
        "\n",
        "# Save it\n",
        "with open(\"/content/drive/MyDrive/SER_Audio_Dataset/label_encoder.pkl\", \"wb\") as f:\n",
        "    pickle.dump(label_encoder, f)\n",
        "\n",
        "print(\"✅ Label encoder recreated and saved.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O49CHnTLy98-",
        "outputId": "73a0d8fc-f10e-4659-be37-d8f28ef26962"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Label encoder recreated and saved.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iP4YeMbpLLzI"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import librosa\n",
        "import numpy as np\n",
        "import pickle\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "# Paths\n",
        "model_path = \"/content/drive/MyDrive/cnn_lstm_ser_model.h5\"\n",
        "encoder_path = \"/content/drive/MyDrive/SER_Audio_Dataset/label_encoder.pkl\"\n",
        "audio_file_path = \"/content/drive/MyDrive/SER_Audio_Dataset/Ravdess/audio_speech_actors_01-24/Actor_01/03-01-01-01-01-01-01.wav\"  # 🔁 Change this\n",
        "\n",
        "# Load model and encoder\n",
        "model = load_model(model_path)\n",
        "with open(encoder_path, \"rb\") as f:\n",
        "    label_encoder = pickle.load(f)\n",
        "\n",
        "# Extract MFCC\n",
        "def extract_mfcc_sequence(path, max_len=173):\n",
        "    y, sr = librosa.load(path, sr=16000)\n",
        "    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=40)\n",
        "    if mfcc.shape[1] < max_len:\n",
        "        pad_width = max_len - mfcc.shape[1]\n",
        "        mfcc = np.pad(mfcc, pad_width=((0, 0), (0, pad_width)), mode='constant')\n",
        "    else:\n",
        "        mfcc = mfcc[:, :max_len]\n",
        "    return mfcc.T\n",
        "\n",
        "# Load audio and preprocess\n",
        "mfcc = extract_mfcc_sequence(audio_file_path)\n",
        "mfcc = np.expand_dims(mfcc, axis=0)  # Add batch dimension\n",
        "\n",
        "# Predict\n",
        "y_pred_prob = model.predict(mfcc)\n",
        "predicted_index = np.argmax(y_pred_prob)\n",
        "predicted_label = label_encoder.inverse_transform([predicted_index])[0]\n",
        "\n",
        "print(\"🎙️ Audio File:\", os.path.basename(audio_file_path))\n",
        "print(\"🔮 Predicted Emotion:\", predicted_label)\n"
      ],
      "metadata": {
        "id": "1aeNObrYKund",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e207d18-539a-4cec-e336-2f7486915114"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 463ms/step\n",
            "🎙️ Audio File: 1001_IOM_HAP_XX.wav\n",
            "🔮 Predicted Emotion: happy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zoeqDmnnKzvD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}